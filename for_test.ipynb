{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# app xml 파일 쪼개기\n",
    "\n",
    "### 각 application이 하나의 xml 파일 그 자체    \n",
    "__=> 하나씩 잘라야 함__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#-*- coding: utf-8 -*-\n",
    "import os\n",
    "import re\n",
    "import csv\n",
    "import sys\n",
    "import xml.etree.ElementTree as et\n",
    "import datetime as dt\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "reload(sys)\n",
    "sys.setdefaultencoding('utf-8')  # to process name of assignee in unicode \n",
    "os.chdir('J:/data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f=open('sample.xml', 'r')\n",
    "endtag_regex = re.compile('^<!DOCTYPE (.*) SYSTEM')\n",
    "endtag = ''\n",
    "doc=''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "line = f.readline()\n",
    "doc += line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "endtag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "endtag_regex.findall(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "endtag = endtag_regex.findall(line) if not endtag else endtag\n",
    "endtag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "not endtag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "terminate = re.compile('^</{0}>'.format(endtag[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "terminate.findall(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "if not ~ else? 이게 뭐지?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if not endtag == True:\n",
    "    endtag = endtag_regex.findall(line)\n",
    "else:\n",
    "    endtag\n",
    "\n",
    "endtag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "아하 이렇게 쓰기 싫어서 그냥 한 줄로 끝냈구만!  \n",
    "함수 이해 완료"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_xml_strings(filename):\n",
    "    \"\"\"\n",
    "    Given a string [filename], opens the file and returns a generator\n",
    "    that yields tuples. A tuple is of format (year, xmldoc string). A tuple\n",
    "    is returned for every valid XML doc in [filename]\n",
    "    \"\"\"\n",
    "    # search for terminating XML tag\n",
    "    endtag_regex = re.compile('^<!DOCTYPE (.*) SYSTEM')\n",
    "    endtag = ''\n",
    "    with open(filename, 'r') as f:\n",
    "        doc = ''  # initialize current XML doc to empty string\n",
    "        for line in f:\n",
    "            doc += line\n",
    "            endtag = endtag_regex.findall(line) if not endtag else endtag   #is there any endtags?\n",
    "            if not endtag:\n",
    "                continue      #if there's no end tag, keep the loop going\n",
    "            terminate = re.compile('^</{0}>'.format(endtag[0]))    #if there's a end tag, then check whether the doc is finished or not\n",
    "            if terminate.findall(line):     #if there's a terminator\n",
    "                yield (doc)                 #export doc and reinitialize a doc\n",
    "                endtag = ''\n",
    "                doc = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sample = extract_xml_strings('sample.xml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for x in sample:\n",
    "    print x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "good~ 이젠 sample.next 해서 각 doc을 parsing, App Np, DN, assignee, class 추출하자"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# xml 자동으로 불러오기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "application xml 파일이 1주일 단위로 구성  \n",
    "ex) ipab20070308, ipab20070315, ...  \n",
    "규칙에 맞게 자동으로 불러오도록 만들자"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "먼저 날짜 다루기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "date = '20070301'\n",
    "date = dt.datetime.strptime(date,'%Y%m%d')\n",
    "date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "date += dt.timedelta(weeks=1)\n",
    "date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "date=date.strftime('%Y%m%d')\n",
    "date"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "filename 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filename = 'ipab'+ date + '.xml'\n",
    "filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sample = extract_xml_strings(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 각 XML에서 원하는 element 뽑기\n",
    "\n",
    "App NO, DN, Assignee, Class, Filed date, Country 필요"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "일단 각각 element가 뭔지 보면 (등장 순서대로)  \n",
    "DN : us-bibliographic-data-application / publication-reference / document-id / doc-number  \n",
    "APP No : us-bibliographic-data-application / application-reference / documnet-id / doc-number  \n",
    "Filed date : us-bibliographic-data-application / application-reference / documnet-id / date  \n",
    "Class : us-bibliographic-data-application / classification-national / main-classification  \n",
    "__** issue : 3digit에서 0 제거 안 함__  ex) 312/29.1 = 312029001  \n",
    "Assignee : us-bibliographic-data-application / assignees / addressbook / orgname  \n",
    "Country : us-bibliographic-data-application / assignees / addressbook / address / country  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "doc1 = sample.next()\n",
    "doc2 = sample.next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "root = et.fromstring(doc1)\n",
    "root"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "list 만들어서 각 정보들 넣은 후, csv로 바꾸자  \n",
    "dn, app no, assignee, country, date 담은 csv  \n",
    "dn, app no, class 담은 csv  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for n in root.iter('doc-number'):\n",
    "    dn = n.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n = root.iter('doc-number')\n",
    "dn = n.next().text\n",
    "appno = (lambda x: x[0:2] + '/' + x[2:8])(n.next().text)\n",
    "print dn, type(dn), appno, type(appno)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# DN, APP NO, country, date 얻기\n",
    "dn = root[0][0][0][1].text\n",
    "appno = (lambda x: x[0:2] + '/' + x[2:8])(root[0][1][0][1].text)\n",
    "date = (lambda x: x[0:4] + '-' + x[4:6] + '-' + x[6:8])(root[0][1][0][2].text)\n",
    "\n",
    "aline = [dn,appno,date]\n",
    "cline = [dn,appno]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Assignee 얻기 (assignee는 복수일 수 있으니 iter 사용)\n",
    "for ass in root.iter('assignee'):\n",
    "    for name in ass.iter(\"orgname\"):\n",
    "        aline.append(name.text)\n",
    "    for country in ass.iter(\"country\"):\n",
    "        aline.append(country.text)\n",
    "\n",
    "aline.append('private') if len(aline)==3 else aline\n",
    "aline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def digitize(cls):\n",
    "    try :\n",
    "        return str(int(cls))\n",
    "    except ValueError:\n",
    "        if cls[0] == '0':\n",
    "            cls=cls.replace('0','')\n",
    "        return cls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def classmaker(cls,line):\n",
    "    cls=cls.replace(' ','0')\n",
    "    if len(cls) < 9:\n",
    "        for i in range(9-len(cls)):\n",
    "            cls += '0'\n",
    "\n",
    "    line.append(digitize(cls[0:3]))\n",
    "    if digitize(cls[6:9])=='0':\n",
    "        line.append(digitize(cls[3:6]))\n",
    "    else:\n",
    "        sub = (lambda x,y: x + '.' + y)(digitize(cls[3:6]),digitize(cls[6:9]))\n",
    "        line.append(sub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Class 얻기 (class는 복수일 수 있으니 iter 사용)\n",
    "for ass in root.iter('main-classification'):\n",
    "    rawclass = ass.text\n",
    "    cline.append(classmaker(rawclass[0:3]))\n",
    "    if not int(rawclass[6:9]):\n",
    "        cline.append(classmaker(rawclass[3:6]))\n",
    "    else:\n",
    "        sub = (lambda x,y: x + '.' + y)(classmaker(rawclass[3:6]),classmaker(rawclass[6:9]))\n",
    "        cline.append(sub)\n",
    "\n",
    "cline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "02 ~ 04랑 05 ~ 13이 구조적으로 완전 다르네"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class AllEntities:\n",
    "    def __getitem__(self, key):\n",
    "        #key is your entity, you can do whatever you want with it here\n",
    "        return key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "parser = et.XMLParser()\n",
    "parser._parser.UseForeignDTD(True)\n",
    "parser.entity = AllEntities()\n",
    "root = et.fromstring(doc1,parser=parser)\n",
    "root"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "date = '20070308'  #initializing date\n",
    "\n",
    "with open('app_profile.csv', 'wb') as prf, open('app_class.csv', 'wb') as cls:\n",
    "    prf_writer = csv.writer(prf,quoting=csv.QUOTE_ALL)\n",
    "    cls_writer = csv.writer(cls,quoting=csv.QUOTE_ALL)\n",
    "    \n",
    "    prf_writer.writerow(['number','id','date','assignee','country'])\n",
    "    cls_writer.writerow(['number','id','mainclass','subclass','further_m','further_s'])\n",
    "    \n",
    "    while not date=='20070315':\n",
    "        filename = 'xmls/ipab'+ date + '.xml'   #making filename\n",
    "        xml = extract_xml_strings(filename)   #load and split whole xml into app_xmls  \n",
    "        for doc in xml:\n",
    "            parser = et.XMLParser()\n",
    "            parser.parser.UseForeignDTD(True)\n",
    "            parser.entity = AllEntities()\n",
    "            root = et.fromstring(doc, parser=parser)\n",
    "            n = root.iter('doc-number')\n",
    "            dn = n.next().text\n",
    "            appno = (lambda x: x[0:2] + '/' + x[2:8])(n.next().text)\n",
    "            \n",
    "            n = root.iter('filing-date')\n",
    "            fdate = (lambda x: x[0:4] + '-' + x[4:6] + '-' + x[6:8])(n.next().text)\n",
    "            \n",
    "            aline = [dn,appno,fdate]\n",
    "            cline = [dn,appno]\n",
    "            \n",
    "            for ass in root.iter('assignee'):   #using iter because of multi-assignee apps \n",
    "                name = ass.iter(\"organization-name\").next().text\n",
    "                aline.append(name)\n",
    "                \n",
    "                for n in ass.iter(\"country-code\"):    #to deal with country-missed assignees\n",
    "                    country = n.text\n",
    "                    aline.append(country)    \n",
    "                \n",
    "            aline.append('private') if len(aline)==3 else aline\n",
    "            \n",
    "            for cls in root.iter('uspc'):  #using iter because of multi-class apps\n",
    "                rawclass = cls[0].text + cls[1].text\n",
    "                classmaker(rawclass,cline)           \n",
    "                          \n",
    "            prf_writer.writerow(aline)\n",
    "            cls_writer.writerow(cline)\n",
    "        \n",
    "        date = dt.datetime.strptime(date,'%Y%m%d')   #Let's move on to next xml\n",
    "        date += dt.timedelta(weeks=1)\n",
    "        date = date.strftime('%Y%m%d')            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "date = '20070315'  #initializing date\n",
    "\n",
    "with open('app_profile.csv', 'ab') as prf, open('app_class.csv', 'ab') as cls:\n",
    "    prf_writer = csv.writer(prf,quoting=csv.QUOTE_ALL)\n",
    "    cls_writer = csv.writer(cls,quoting=csv.QUOTE_ALL)\n",
    "    \n",
    "    while not date=='20070329':\n",
    "        filename = 'xmls/ipab'+ date + '.xml'   #making filename\n",
    "        xml = extract_xml_strings(filename)   #load and split whole xml into app_xmls  \n",
    "        for doc in xml:\n",
    "            root = et.fromstring(doc)\n",
    "            n = root.iter('doc-number')\n",
    "            dn = n.next().text\n",
    "            appno = (lambda x: x[0:2] + '/' + x[2:8])(n.next().text)\n",
    "            fdate = (lambda x: x[0:4] + '-' + x[4:6] + '-' + x[6:8])(root[0][1][0][2].text)\n",
    "                \n",
    "            aline = [dn,appno,fdate]\n",
    "            cline = [dn,appno]\n",
    "            \n",
    "            for ass in root.iter('assignee'):   #using iter because of multi-assignee apps \n",
    "                name = ass.iter(\"orgname\").next().text\n",
    "                aline.append(name)\n",
    "                \n",
    "                for n in ass.iter(\"country\"):    #to deal with country-missed assignees\n",
    "                    country = n.text\n",
    "                    aline.append(country)    \n",
    "                \n",
    "            aline.append('private') if len(aline)==3 else aline\n",
    "            \n",
    "            for cls in root.iter('main-classification'):  #using iter because of multi-class apps\n",
    "                rawclass = cls.text\n",
    "                classmaker(rawclass,cline)\n",
    "            \n",
    "            for cls in root.iter('further-classification'):  #using iter because of multi-class apps\n",
    "                rawclass = cls.text\n",
    "                classmaker(rawclass,cline)\n",
    "                          \n",
    "            prf_writer.writerow(aline)\n",
    "            cls_writer.writerow(cline)\n",
    "        \n",
    "        date = dt.datetime.strptime(date,'%Y%m%d')   #Let's move on to next xml\n",
    "        date += dt.timedelta(weeks=1)\n",
    "        date = date.strftime('%Y%m%d')     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# numpy로 grant dummy 처리하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "merge 후 granted_dum_i, granted_dum_i 중 __하나만 NaN 일 때__ 정상  \n",
    "둘 다 NaN일 경우 (matching 실패) or 둘 다 NaN 아닐 경우 (matching 중복) 처리 필요  \n",
    "=> numpy array로 처리 후 다시 갖다 붙이자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([(  1.,  nan), (  0.,  nan), ( nan,  nan), ( nan,  nan),\n",
       "       ( nan,   0.), ( nan,   1.), ( nan,   2.), (  1.,   1.),\n",
       "       (  0.,   0.), (  1.,   2.), (  0.,   2.), (  1.,   2.),\n",
       "       (  0.,   2.), (  1.,   1.), (  1.,   0.), (  0.,   1.),\n",
       "       (  0.,   1.), (  1.,  nan), (  0.,  nan)], \n",
       "      dtype=[('granted_dum_i', '<f8'), ('granted_dum_n', '<f8')])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dummy 뒷처리\n",
    "\n",
    "dum = np.genfromtxt('grtdum_temp.csv',delimiter=',',dtype=float,names=True)\n",
    "dum\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arr=dum[0]\n",
    "arr[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def granted_dum(arr):\n",
    "    a = arr[0] + arr[1]\n",
    "    if math.isnan(a):\n",
    "        if not math.isnan(arr[0]):\n",
    "            a = arr[0]\n",
    "        else:\n",
    "            a = arr[1] if not math.isnan(arr[1]) else a\n",
    "            a = np.nan if a==2 else a\n",
    "     \n",
    "    else:\n",
    "        if a==2:\n",
    "            if arr[0]==arr[1]:\n",
    "                a=1\n",
    "            \n",
    "            else:\n",
    "                a=0\n",
    "        \n",
    "        elif a==3:\n",
    "            a=1\n",
    "            \n",
    "    return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vfunc = np.vectorize(granted_dum)\n",
    "result = vfunc(dum)\n",
    "np.savetxt('result.csv', result,delimiter=',', fmt='%1.1f', header='granted_dum', comments='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  1.,   0.,  nan,  nan,   0.,   1.,  nan,   1.,   0.,   1.,   0.,\n",
       "         1.,   0.,   1.,   1.,   1.,   1.,   1.,   0.])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.savetxt('result.csv', result,delimiter=',', fmt='%1.1f', header='granted_dum', comments='')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# df.iterrow 사용해보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import pandas as pd\n",
    "\n",
    "os.chdir('J:/data')\n",
    "\n",
    "df = pd.read_csv('ass.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "StopIteration",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mStopIteration\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-4eded61367cf>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      8\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mrow\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'patent_id'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mline\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m             \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'assignee'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'private'\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m             \u001b[0mline\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mreader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mStopIteration\u001b[0m: "
     ]
    }
   ],
   "source": [
    "with open('pid_private.csv','rb') as prv:\n",
    "    reader = csv.reader(prv)\n",
    "    reader.next()\n",
    "    line = reader.next()\n",
    "        \n",
    "    for i, row in df.iterrows():\n",
    "        a=line[0]\n",
    "        if row['patent_id'] == int(line[0]):\n",
    "            df.loc[i,'assignee'] = 'private'\n",
    "            line = reader.next()           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>patent_id</th>\n",
       "      <th>assignee</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>private</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>sanvk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>vrji</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>vkf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>regh</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>private</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>gnf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>private</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>private</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>fnj</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   patent_id assignee\n",
       "0          1  private\n",
       "1          2    sanvk\n",
       "2          3     vrji\n",
       "3          4      vkf\n",
       "4          5     regh\n",
       "5          6  private\n",
       "6          7      gnf\n",
       "7          8  private\n",
       "8          9  private\n",
       "9         10      fnj"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Patent xml에서 appl citaiton 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#-*- coding: utf-8 -*-\n",
    "import os\n",
    "import re\n",
    "import csv\n",
    "import sys\n",
    "import xml.etree.ElementTree as et\n",
    "import datetime as dt\n",
    "\n",
    "reload(sys)\n",
    "sys.setdefaultencoding('utf-8')  # to process name of assignee in unicode \n",
    "os.chdir('J:/data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_xml_strings(filename):\n",
    "    \"\"\"\n",
    "    Given a string [filename], opens the file and returns a generator\n",
    "    that yields tuples. A tuple is of format (year, xmldoc string). A tuple\n",
    "    is returned for every valid XML doc in [filename]\n",
    "    \"\"\"\n",
    "    # search for terminating XML tag\n",
    "    endtag_regex = re.compile('^<!DOCTYPE (.*) SYSTEM')\n",
    "    endtag = ''\n",
    "    with open(filename, 'r') as f:\n",
    "        doc = ''  # initialize current XML doc to empty string\n",
    "        for line in f:\n",
    "            doc += line\n",
    "            endtag = endtag_regex.findall(line) if not endtag else endtag   #is there any endtags?\n",
    "            if not endtag:\n",
    "                continue      #if there's no end tag, keep the loop going\n",
    "            terminate = re.compile('^</{0}>'.format(endtag[0]))    #if there's a end tag, then check whether the doc is finished or not\n",
    "            if terminate.findall(line):     #if there's a terminator\n",
    "                yield (doc)                 #export doc and reinitialize a doc\n",
    "                endtag = ''\n",
    "                doc = ''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 추출할 element\n",
    "\n",
    "patent_id : 첫 doc-number  \n",
    "app_cited : patcit / document-id / country='US' & len(doc-number) > 7  \n",
    "* ** country != 'US'에서 탈출 (다음 문서로 넘너가면 됨) **\n",
    "\n",
    "examiner: citation / category  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'D7643386'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample = extract_xml_strings('ipgb20100105.xml')\n",
    "doc1=sample.next()\n",
    "root = et.fromstring(doc1)\n",
    "n = root.iter('doc-number')\n",
    "pid = n.next().text\n",
    "pid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pid의 문제: 맨 앞이 D로 시작하는 애들 때문에 D없는 애들 앞에 0이 붙음  \n",
    "but, 우리가 관심있는 특허는 D로 시작하는 애들 없음  \n",
    "**=> int 만들어서 0 떼고 value Error 뜨면 바로 다음 doc으로 넘겨버리자**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([7643387], 2, 2, 2, 1, 1)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample = extract_xml_strings('ipgb20100105.xml')\n",
    "a=[]\n",
    "b=0\n",
    "c=0\n",
    "d=0\n",
    "e=0\n",
    "f=0\n",
    "for doc in sample:\n",
    "    root = et.fromstring(doc)\n",
    "    b += 1\n",
    "    n = root.iter('doc-number')\n",
    "    c += 1\n",
    "    pid = n.next().text\n",
    "    d += 1\n",
    "    try:\n",
    "        pid = int(pid)\n",
    "        e += 1\n",
    "        a.append(pid)\n",
    "\n",
    "    except ValueError:\n",
    "        f += 1\n",
    "        pass\n",
    "        \n",
    "a,b,c,d,e,f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cit id의 문제: 연도와 번호 사이의 /  \n",
    "**=> / 를 공백으로 만들자**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('5673247', 'cited by examiner')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cit=root.iter('citation')\n",
    "k = cit.next()\n",
    "did = k[0][0]\n",
    "cited = k[1].text\n",
    "a=did.find('doc-number').text\n",
    "a, cited"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cit_dum = 1 if cited.split()[-1] == 'examiner' else 0\n",
    "cit_dum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'5673247'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a=a.replace('/','')\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "date = '20100105'  #initializing date\n",
    "\n",
    "with open('app_citation.csv', 'wb') as cit:\n",
    "    writer = csv.writer(cit,quoting=csv.QUOTE_ALL)\n",
    "    writer.writerow(['patent_id','app_cited','by_examiner'])\n",
    "    \n",
    "    while not date=='20100126':\n",
    "        filename = 'xmls/ipgb'+ date + '.xml'   #making filename\n",
    "        xml = extract_xml_strings(filename)   #load and split whole xml into app_xmls  \n",
    "        for doc in xml:\n",
    "            root = et.fromstring(doc)\n",
    "            n = root.iter('doc-number')\n",
    "            pid = n.next().text\n",
    "\n",
    "            try:\n",
    "                pid = int(pid)                          \n",
    "                for cit in root.iter('citation'):   #using iter because of multi-assignee apps \n",
    "                    did = cit[0][0]\n",
    "                    try:\n",
    "                        cited = cit[1].text              \n",
    "                        try:\n",
    "                            if did.find('country').text != 'US':  #if foreign cit, go to next doc\n",
    "                                break \n",
    "\n",
    "                            cid = did.find('doc-number').text\n",
    "                            if len(cid) > 8:     #if app cit, go to next doc\n",
    "                                break\n",
    "                            \n",
    "                            cit_dum = 1 if cited.split()[-1] == 'examiner' else 0\n",
    "\n",
    "                            writer.writerow([pid,cid,cit_dum])\n",
    "\n",
    "                        except AttributeError:         #if other cit, go to next doc\n",
    "                            break\n",
    "                        \n",
    "                    except IndexError:    #if other cit, go to next doc\n",
    "                        break\n",
    "                        \n",
    "            except ValueError:      #if pid != int, go to the next doc\n",
    "                continue    \n",
    "\n",
    "        date = dt.datetime.strptime(date,'%Y%m%d')   #Let's move on to next xml\n",
    "        date += dt.timedelta(weeks=1)\n",
    "        date = date.strftime('%Y%m%d')   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'20100126'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "date"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# regex 써서 patent infringement case 개수 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import os\n",
    "import re\n",
    "\n",
    "os.chdir('J:/data')\n",
    "\n",
    "patent_regex = re.compile('.* Patent Infringement$')\n",
    "cnt = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('cases.csv', 'rb') as f:\n",
    "    reader = csv.reader(f)\n",
    "    header = reader.next()\n",
    "    i = header.index('case_cause')\n",
    "    for line in reader:\n",
    "        if patent_regex.findall(line[i]):\n",
    "            cnt+=1\n",
    "cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 기업간 proximity matrix 제작"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "import os\n",
    "import math\n",
    "from math import sqrt\n",
    "\n",
    "os.chdir('J:/data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def prox_numer(l1,l2):\n",
    "    cnt = 0.0\n",
    "    for i in xrange(len(l1)):\n",
    "        if l1[i]!=0 and l2[i]!=0:\n",
    "            w= float(l1[i]) * l2[i]\n",
    "            cnt += w\n",
    "    \n",
    "    return round(cnt,3) \n",
    "\n",
    "def prox_denom(l1,l2):\n",
    "    a = sqrt(sum(map(lambda x: x**2, l1)))\n",
    "    b = sqrt(sum(map(lambda x: x**2, l2)))\n",
    "    \n",
    "    return a*b \n",
    "\n",
    "def proximity(l1,l2):\n",
    "    return prox_numer(l1,l2) / prox_denom(l1,l2)\n",
    "\n",
    "def get_prox(l1,arr):\n",
    "    sim = []\n",
    "    for row in arr:\n",
    "        sim.append(proximity(l1,row))\n",
    "        \n",
    "    return sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "arr = pd.read_csv('sample.csv').values\n",
    "\n",
    "header = arr[:,0].tolist()\n",
    "header = ['gvkey'] + map(int,header)\n",
    "\n",
    "arr = np.delete(arr,0,1)\n",
    "\n",
    "with open('counting.csv','wb') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(header)\n",
    "    \n",
    "    i=1  \n",
    "    for row in arr:        \n",
    "        line = get_prox(row,arr)\n",
    "        line = [header[i]] + line             \n",
    "        writer.writerow(line)\n",
    "        i += 1"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
